# Lenisha

> A GPTâ€‘v1 architecture â€” a minimal/educational implementation of a GPTâ€‘style language model.

## âœ… What is Lenisha

Lenisha is a lightweight GPTâ€‘v1 style architecture written in a Jupyter Notebook (`gpt.ipynb`).  
It is designed for experimentation and learning â€” to help you understand how a transformerâ€‘based language model works â€œunder the hood,â€ without the complexity of large industrial codebases.  

## ğŸ§  Motivation & Goals

- To learn and demonstrate the core mechanics of transformerâ€‘based language modeling.  
- To give other students / developers a simple, easy-to-follow reference implementation.  
- To explore custom modifications/experiments on a manageable scale (since this repo is small and selfâ€‘contained).  

## ğŸ“‚ Repository Structure

```
Lenisha/
â”œâ”€â”€ gpt.ipynb       â† Jupyter Notebook containing model code, training and inference logic
```

## Getting Started

### Prerequisites

- Python 3.x  
- Jupyter Notebook (or Jupyter Lab)  
- Typical ML / DL Python libraries (e.g. `torch`, `numpy`, etc.) â€” install based on your notebookâ€™s imports  

### Running the Project

1. Clone the repo  
   ```bash
   git clone https://github.com/SulavGyawali/Lenisha.git
   cd Lenisha
   ```  
2. Open `gpt.ipynb` in Jupyter Notebook / Lab  
3. Follow the notebook: configure hyperâ€‘parameters, run training / inference cells  

## âœ¨ What You Can Do

- Step through a minimal GPTâ€‘v1 implementation to understand attention, tokenization, positional embeddings, forward pass, etc.  
- Modify architecture â€” change number of layers, embedding size, context length, etc.  
- Train on a small dataset for toy experiments (since the code is small).  
- Use as a learning base or starting point for custom NLP experiments.  

## âš ï¸ Limitations / Known Issues

- This is **not** a production-grade model â€” itâ€™s purely educational / experimental.  
- Trained results (if using small datasets) may not reflect real-world performance / language fluency.  
- No extensive dataset preprocessing, tokenizers, evaluation pipeline, or deployment setup (as of now).  

## ğŸš€ Future Ideas / Toâ€‘Dos

- Add a proper tokenizer / preprocessing pipeline.  
- Support saving & loading model weights.  
- Add training & evaluation scripts (outside notebook) for scalability.  
- Integrate example dataset and show sample generation outputs.  
- Improve docs with usage examples, sample outputs, limitations clearly listed.  




